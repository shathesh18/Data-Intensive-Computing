{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, CountVectorizer\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Data preprocessing\") \\\n",
    "        .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "        .getOrCreate()\n",
    "dataframe = spark.read.csv(\"/home/cse587/Downloads/diccsvs/train.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n",
    "dataframe = dataframe.na.drop(subset=[\"genre\",\"plot\",\"movie_id\"])\n",
    "dataframe.printSchema()\n",
    "df_mapping = spark.read.csv(\"/home/cse587/Downloads/diccsvs/mapping.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clean text\n",
    "df_clean = dataframe.select('movie_id', 'movie_name', (lower(regexp_replace('plot',\"[^a-zA-Z\\\\s]\",\"\")).alias('plot')), (lower(regexp_replace(\"genre\",\"[^a-zA-Z\\-/,\\\\s]\",\"\")).alias(\"genre\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def replacelabel(x):\n",
    "    test = x.split(\", \")\n",
    "    num_label = []\n",
    "    if(len(test)<1):\n",
    "        return num_label\n",
    "    for label in test:\n",
    "        if label == 'drama':\n",
    "            num_label.append(0)\n",
    "        elif label == 'comedy':\n",
    "            num_label.append(1)\n",
    "        elif label == 'romance film':\n",
    "            num_label.append(2)\n",
    "        elif label ==  'thriller':\n",
    "            num_label.append(3)\n",
    "        elif label == 'action': \n",
    "            num_label.append(4)\n",
    "        elif label == 'world cinema':\n",
    "            num_label.append(5)\n",
    "        elif label == 'crime fiction':\n",
    "            num_label.append(6)\n",
    "        elif label == 'horror':\n",
    "            num_label.append(7)\n",
    "        elif label == 'black-and-white':\n",
    "            num_label.append(8)\n",
    "        elif label == 'indie':\n",
    "            num_label.append(9)\n",
    "        elif label == 'action/adventure':\n",
    "            num_label.append(10)\n",
    "        elif label == 'adventure':\n",
    "            num_label.append(11)\n",
    "        elif label == 'family film':\n",
    "            num_label.append(12)\n",
    "        elif label == 'short film':\n",
    "            num_label.append(13)\n",
    "        elif label == 'romantic drama':\n",
    "            num_label.append(14)\n",
    "        elif label == 'animation':\n",
    "            num_label.append(15)\n",
    "        elif label == 'musical':\n",
    "            num_label.append(16)\n",
    "        elif label == 'science fiction':\n",
    "            num_label.append(17)\n",
    "        elif label == 'mystery':\n",
    "            num_label.append(18)\n",
    "        elif label == 'romantic comedy':\n",
    "            num_label.append(19)\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_udf = udf(replacelabel, ArrayType(IntegerType()))\n",
    "df_clean = df_clean.withColumn('genre_value',label_udf(df_clean.genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- genre_value: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Tokenize Plot Text\n",
    "tokenizer = Tokenizer(inputCol = 'plot', outputCol = 'plot_token')\n",
    "df_words_token = tokenizer.transform(df_clean).select(\"movie_id\",\"movie_name\",\"plot_token\",\"genre\",\"genre_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove StopWords\n",
    "remover = StopWordsRemover(inputCol = 'plot_token', outputCol = 'plot_clean')\n",
    "df_words_token_rem_stopwor = remover.transform(df_words_token).select(\"movie_id\",\"movie_name\",\"plot_clean\",\"genre\",\"genre_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Text Stemming\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stem_udf = udf(lambda tokens : [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_words_token_rem_stopwor.withColumn(\"words_stemmed\" ,stem_udf(\"plot_clean\")).select('movie_id',\"words_stemmed\",\"genre\",\"genre_value\")\n",
    "df_stemmed = df_stemmed.withColumnRenamed(\"words_stemmed\",\"plot\")\n",
    "df_stemmed = df_stemmed.withColumnRenamed(\"genre_value\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stemmed = df_stemmed.withColumn(\"col_0\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_1\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_2\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_3\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_4\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_5\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_6\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_7\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_8\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_9\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_10\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_11\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_12\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_13\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_14\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_15\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_16\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_17\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_18\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_19\",lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name='movie_id', dataType=tp.StringType(), nullable=True),\n",
    "    tp.StructField(name='plot', dataType=tp.ArrayType(StringType()), nullable=True),\n",
    "    tp.StructField(name='genre', dataType=tp.StringType(), nullable=True),\n",
    "    tp.StructField(name='label', dataType=tp.ArrayType(IntegerType()), nullable=True),\n",
    "    tp.StructField(name='col_0', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_1', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_2', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_3', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_4', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_5', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_6', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_7', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_8', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_9', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_10', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_11', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_12', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_13', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_14', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_15', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_16', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_17', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_18', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_19', dataType=tp.IntegerType(), nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = df_stemmed.select(\"*\").toPandas()\n",
    "for index, row in result_df.iterrows():\n",
    "    label_arr = row['label']\n",
    "    for i in label_arr:\n",
    "        if i == 0:\n",
    "            result_df.loc[index, \"col_0\"] = 1\n",
    "        if i == 1:\n",
    "            result_df.loc[index, \"col_1\"] = 1\n",
    "        if i == 2:\n",
    "            result_df.loc[index, \"col_2\"] = 1\n",
    "        if i == 3:\n",
    "            result_df.loc[index, \"col_3\"] = 1\n",
    "        if i == 4:\n",
    "            result_df.loc[index, \"col_4\"] = 1\n",
    "        if i == 5:\n",
    "            result_df.loc[index, \"col_5\"] = 1\n",
    "        if i == 6:\n",
    "            result_df.loc[index, \"col_6\"] = 1\n",
    "        if i == 7:\n",
    "            result_df.loc[index, \"col_7\"] = 1\n",
    "        if i == 8:\n",
    "            result_df.loc[index, \"col_8\"] = 1\n",
    "        if i == 9:\n",
    "            result_df.loc[index, \"col_9\"] = 1\n",
    "        if i == 10:\n",
    "            result_df.loc[index, \"col_10\"] = 1\n",
    "        if i == 11:\n",
    "            result_df.loc[index, \"col_11\"] = 1\n",
    "        if i == 12:\n",
    "            result_df.loc[index, \"col_12\"] = 1\n",
    "        if i == 13:\n",
    "            result_df.loc[index, \"col_13\"] = 1\n",
    "        if i == 14:\n",
    "            result_df.loc[index, \"col_14\"] = 1\n",
    "        if i == 15:\n",
    "            result_df.loc[index, \"col_15\"] = 1\n",
    "        if i == 16:\n",
    "            result_df.loc[index, \"col_16\"] = 1\n",
    "        if i == 17:\n",
    "            result_df.loc[index, \"col_17\"] = 1\n",
    "        if i == 18:\n",
    "            result_df.loc[index, \"col_18\"] = 1\n",
    "        if i == 19:\n",
    "            result_df.loc[index, \"col_19\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stemmed = spark.createDataFrame(result_df, schema = my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movie_id='23890098', plot=['shlykov', 'hardwork', 'taxi', 'driver', 'lyosha', 'saxophonist', 'develop', 'bizarr', 'loveh', 'relationship', 'despit', 'prejudic', 'realiz', 'arent', 'differ'], genre='world cinema, drama', label=[5, 0], col_0=1, col_1=0, col_2=0, col_3=0, col_4=0, col_5=1, col_6=0, col_7=0, col_8=0, col_9=0, col_10=0, col_11=0, col_12=0, col_13=0, col_14=0, col_15=0, col_16=0, col_17=0, col_18=0, col_19=0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movie_id='23890098', plot=['shlykov', 'hardwork', 'taxi', 'driver', 'lyosha', 'saxophonist', 'develop', 'bizarr', 'loveh', 'relationship', 'despit', 'prejudic', 'realiz', 'arent', 'differ'], genre='world cinema, drama', label=[5, 0], col_0=1, col_1=0, col_2=0, col_3=0, col_4=0, col_5=1, col_6=0, col_7=0, col_8=0, col_9=0, col_10=0, col_11=0, col_12=0, col_13=0, col_14=0, col_15=0, col_16=0, col_17=0, col_18=0, col_19=0, plot_length=['shlykov', 'hardwork', 'taxi', 'driver', 'lyosha', 'saxophonist', 'develop', 'bizarr', 'loveh', 'relationship', 'despit', 'prejudic', 'realiz', 'arent', 'differ'])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter words whose length is greater than 3\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) > 3], ArrayType(StringType()))\n",
    "df_stemmed = df_stemmed.withColumn('plot_length', filter_length_udf(col('plot')))\n",
    "data = df_stemmed.select(\"*\")\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol = \"plot_length\", outputCol = \"features\", minDF= 2.0)\n",
    "cv_model = cv.fit(data)\n",
    "cv_result = cv_model.transform(data)\n",
    "cv_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- plot: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- label: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- col_0: integer (nullable = true)\n",
      " |-- col_1: integer (nullable = true)\n",
      " |-- col_2: integer (nullable = true)\n",
      " |-- col_3: integer (nullable = true)\n",
      " |-- col_4: integer (nullable = true)\n",
      " |-- col_5: integer (nullable = true)\n",
      " |-- col_6: integer (nullable = true)\n",
      " |-- col_7: integer (nullable = true)\n",
      " |-- col_8: integer (nullable = true)\n",
      " |-- col_9: integer (nullable = true)\n",
      " |-- col_10: integer (nullable = true)\n",
      " |-- col_11: integer (nullable = true)\n",
      " |-- col_12: integer (nullable = true)\n",
      " |-- col_13: integer (nullable = true)\n",
      " |-- col_14: integer (nullable = true)\n",
      " |-- col_15: integer (nullable = true)\n",
      " |-- col_16: integer (nullable = true)\n",
      " |-- col_17: integer (nullable = true)\n",
      " |-- col_18: integer (nullable = true)\n",
      " |-- col_19: integer (nullable = true)\n",
      " |-- plot_length: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42783\n"
     ]
    }
   ],
   "source": [
    "print(len(cv_model.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, OneVsRest\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998296312964094\n",
      "0.9990356488476003\n",
      "0.9995821145006268\n",
      "0.9997749847311067\n",
      "0.9998392748079334\n",
      "0.9997749847311067\n",
      "0.9997428396926934\n",
      "0.9999357099231734\n",
      "0.9998392748079334\n",
      "0.9999035648847601\n",
      "1.0\n",
      "0.9999035648847601\n",
      "0.9998714198463468\n",
      "0.9996785496158668\n",
      "0.9999357099231734\n",
      "1.0\n",
      "0.9999357099231734\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_0', fitIntercept = True)\n",
    "lrModel = lr.fit(cv_result)\n",
    "print(lrModel.summary.accuracy)\n",
    "\n",
    "\n",
    "\n",
    "lr1 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_1', fitIntercept = True)\n",
    "lrModel1 = lr1.fit(cv_result)\n",
    "print(lrModel1.summary.accuracy)\n",
    "\n",
    "lr2 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_2', fitIntercept = True)\n",
    "lrModel2 = lr2.fit(cv_result)\n",
    "print(lrModel2.summary.accuracy)\n",
    "\n",
    "lr3 = LogisticRegression(maxIter=20, elasticNetParam = 0, featuresCol = 'features', labelCol='col_3', fitIntercept = True)\n",
    "lrModel3 = lr3.fit(cv_result)\n",
    "print(lrModel3.summary.accuracy)\n",
    "\n",
    "lr4 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_4', fitIntercept = True)\n",
    "lrModel4 = lr4.fit(cv_result)\n",
    "print(lrModel4.summary.accuracy)\n",
    "\n",
    "lr5 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_5', fitIntercept = True)\n",
    "lrModel5 = lr5.fit(cv_result)\n",
    "print(lrModel5.summary.accuracy)\n",
    "\n",
    "lr6 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_6', fitIntercept = True)\n",
    "lrModel6 = lr6.fit(cv_result)\n",
    "print(lrModel6.summary.accuracy)\n",
    "\n",
    "lr7 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_7', fitIntercept = True)\n",
    "lrModel7 = lr7.fit(cv_result)\n",
    "print(lrModel7.summary.accuracy)\n",
    "\n",
    "lr8 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_8', fitIntercept = True)\n",
    "lrModel8 = lr8.fit(cv_result)\n",
    "print(lrModel8.summary.accuracy)\n",
    "\n",
    "lr9 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_9', fitIntercept = True)\n",
    "lrModel9 = lr9.fit(cv_result)\n",
    "print(lrModel9.summary.accuracy)\n",
    "\n",
    "lr10 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_10', fitIntercept = True)\n",
    "lrModel10 = lr10.fit(cv_result)\n",
    "print(lrModel10.summary.accuracy)\n",
    "\n",
    "lr11 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_11', fitIntercept = True)\n",
    "lrModel11 = lr11.fit(cv_result)\n",
    "print(lrModel11.summary.accuracy)\n",
    "\n",
    "lr12 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_12', fitIntercept = True)\n",
    "lrModel12 = lr12.fit(cv_result)\n",
    "print(lrModel12.summary.accuracy)\n",
    "\n",
    "lr13 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_13', fitIntercept = True)\n",
    "lrModel13 = lr13.fit(cv_result)\n",
    "print(lrModel13.summary.accuracy)\n",
    "\n",
    "lr14 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_14', fitIntercept = True)\n",
    "lrModel14 = lr14.fit(cv_result)\n",
    "print(lrModel14.summary.accuracy)\n",
    "\n",
    "lr15 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_15', fitIntercept = True)\n",
    "lrModel15 = lr15.fit(cv_result)\n",
    "print(lrModel15.summary.accuracy)\n",
    "\n",
    "lr16 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_16', fitIntercept = True)\n",
    "lrModel16 = lr16.fit(cv_result)\n",
    "print(lrModel16.summary.accuracy)\n",
    "\n",
    "lr17 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_17', fitIntercept = True)\n",
    "lrModel17 = lr17.fit(cv_result)\n",
    "print(lrModel17.summary.accuracy)\n",
    "\n",
    "lr18 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_18', fitIntercept = True)\n",
    "lrModel18 = lr18.fit(cv_result)\n",
    "print(lrModel18.summary.accuracy)\n",
    "\n",
    "lr19 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_19', fitIntercept = True)\n",
    "lrModel19 = lr19.fit(cv_result)\n",
    "print(lrModel19.summary.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"paramGrid = (ParamGridBuilder().addGrid(lr.regParam,[0.1,0.3,0.5])\\\n",
    "             .addGrid(lr.elasticNetParam, [0.0,0.1,0.2])\\\n",
    "             .addGrid(lr.labelCol, 'col_0')\n",
    "            .addGrid(lr.maxIter, [10,20,50]).build())\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol = 'prediction')\n",
    "cv = CrossValidator(estimator=lr, evaluator=evaluator, estimatorParamMaps = paramGrid, numFolds=5)\n",
    "cvModel = cv.fit(cv_result)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9984891831945739\n"
     ]
    }
   ],
   "source": [
    "\"\"\"idf = IDF(inputCol=\"rawfeatures\", outputCol = \"features\")\n",
    "idfModel = idf.fit(cv_result)\n",
    "rescaledData = idfModel.transform(cv_result)\n",
    "idfModel = lr.fit(rescaledData)\n",
    "print(idfModel.summary.accuracy)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994856793853869\n"
     ]
    }
   ],
   "source": [
    "\"\"\"idfModel1 = lr1.fit(rescaledData)\n",
    "print(idfModel1.summary.accuracy)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"rf = RandomForestClassifier(featuresCol = 'features', labelCol='col_0', numTrees = 10, maxDepth = 4)\n",
    "rfModel = rf.fit(cv_result)\n",
    "print(rfModel.summary.accuracy)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.csv(\"/home/cse587/Downloads/diccsvs/test.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n",
    "test_df = test_df.na.drop(subset=[\"plot\",\"movie_id\"])\n",
    "test_df.printSchema()\n",
    "\n",
    "#clean text\n",
    "test_df_clean = test_df.select('movie_id', 'movie_name', (lower(regexp_replace('plot',\"[^a-zA-Z\\\\s]\",\"\")).alias('plot')))\n",
    "\n",
    "#Tokenize Plot Text\n",
    "test_df_words_token = tokenizer.transform(test_df_clean).select(\"movie_id\",\"movie_name\",\"plot_token\")\n",
    "\n",
    "#Remove StopWords\n",
    "test_df_words_token_rem_stopwor = remover.transform(test_df_words_token).select(\"movie_id\",\"movie_name\",\"plot_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Text Stemming\n",
    "test_df_stemmed = test_df_words_token_rem_stopwor.withColumn(\"words_stemmed\" ,stem_udf(\"plot_clean\")).select('movie_id',\"words_stemmed\")\n",
    "test_df_stemmed = test_df_stemmed.withColumnRenamed(\"words_stemmed\",\"plot\")\n",
    "\n",
    "test_df_stemmed = test_df_stemmed.withColumn('plot_length', filter_length_udf(col('plot')))\n",
    "test_data = test_df_stemmed.select(\"movie_id\",\"plot\",\"plot_length\")\n",
    "#test_data.select('plot','plot_length').head(1)\n",
    "test_data = test_df_stemmed.select(\"movie_id\",\"plot\",\"plot_length\")\n",
    "#Count Vectorizer\n",
    "#test_cv_result = cv_model.transform(test_data)\n",
    "#TF-IDF\n",
    "test_cv_result = pipefit.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cv_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test_cv_result)\n",
    "#t_predictions = predictions.select(\"*\").toPandas()\n",
    "predictions1 = lrModel1.transform(test_cv_result)\n",
    "#t1_predictions = predictions1.select(\"*\").toPandas()\n",
    "predictions2 = lrModel2.transform(test_cv_result)\n",
    "#t2_predictions = predictions2.select(\"*\").toPandas()\n",
    "predictions3 = lrModel3.transform(test_cv_result)\n",
    "#t3_predictions = predictions3.select(\"*\").toPandas()\n",
    "predictions4 = lrModel4.transform(test_cv_result)\n",
    "#t4_predictions = predictions4.select(\"*\").toPandas()\n",
    "predictions5 = lrModel5.transform(test_cv_result)\n",
    "#t5_predictions = predictions5.select(\"*\").toPandas()\n",
    "predictions6 = lrModel6.transform(test_cv_result)\n",
    "#t6_predictions = predictions6.select(\"*\").toPandas()\n",
    "predictions7 = lrModel7.transform(test_cv_result)\n",
    "#t7_predictions = predictions7.select(\"*\").toPandas()\n",
    "predictions8 = lrModel8.transform(test_cv_result)\n",
    "#t8_predictions = predictions8.select(\"*\").toPandas()\n",
    "predictions9 = lrModel9.transform(test_cv_result)\n",
    "#t9_predictions = predictions9.select(\"*\").toPandas()\n",
    "predictions10 = lrModel10.transform(test_cv_result)\n",
    "#t10_predictions = predictions10.select(\"*\").toPandas()\n",
    "predictions11 = lrModel11.transform(test_cv_result)\n",
    "#t11_predictions = predictions11.select(\"*\").toPandas()\n",
    "predictions12 = lrModel12.transform(test_cv_result)\n",
    "#t12_predictions = predictions12.select(\"*\").toPandas()\n",
    "predictions13 = lrModel13.transform(test_cv_result)\n",
    "#t13_predictions = predictions13.select(\"*\").toPandas()\n",
    "predictions14 = lrModel14.transform(test_cv_result)\n",
    "#t14_predictions = predictions14.select(\"*\").toPandas()\n",
    "predictions15 = lrModel15.transform(test_cv_result)\n",
    "#t15_predictions = predictions15.select(\"*\").toPandas()\n",
    "predictions16 = lrModel16.transform(test_cv_result)\n",
    "#t16_predictions = predictions16.select(\"*\").toPandas()\n",
    "predictions17 = lrModel17.transform(test_cv_result)\n",
    "#t17_predictions = predictions17.select(\"*\").toPandas()\n",
    "predictions18 = lrModel18.transform(test_cv_result)\n",
    "#t18_predictions = predictions18.select(\"*\").toPandas()\n",
    "predictions19 = lrModel19.transform(test_cv_result)\n",
    "#t19_predictions = predictions19.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0.0)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions12.select('prediction').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enna\n"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "movie_id = predictions.select(F.collect_list('movie_id')).first()[0]\n",
    "pred1 = predictions.select(F.collect_list('prediction')).first()[0]\n",
    "pred2 = predictions1.select(F.collect_list('prediction')).first()[0]\n",
    "pred3 = predictions2.select(F.collect_list('prediction')).first()[0]\n",
    "pred4 = predictions3.select(F.collect_list('prediction')).first()[0]\n",
    "pred5 = predictions4.select(F.collect_list('prediction')).first()[0]\n",
    "pred6 = predictions5.select(F.collect_list('prediction')).first()[0]\n",
    "pred7 = predictions6.select(F.collect_list('prediction')).first()[0]\n",
    "print(\"enna\")\n",
    "pred8 = predictions7.select(F.collect_list('prediction')).first()[0]\n",
    "pred9 = predictions8.select(F.collect_list('prediction')).first()[0]\n",
    "pred10 = predictions9.select(F.collect_list('prediction')).first()[0]\n",
    "pred11 = predictions10.select(F.collect_list('prediction')).first()[0]\n",
    "pred12 = predictions11.select(F.collect_list('prediction')).first()[0]\n",
    "pred13 = predictions12.select(F.collect_list('prediction')).first()[0]\n",
    "pred14 = predictions13.select(F.collect_list('prediction')).first()[0]\n",
    "pred15 = predictions14.select(F.collect_list('prediction')).first()[0]\n",
    "pred16 = predictions15.select(F.collect_list('prediction')).first()[0]\n",
    "pred17 = predictions16.select(F.collect_list('prediction')).first()[0]\n",
    "pred18 = predictions17.select(F.collect_list('prediction')).first()[0]\n",
    "pred19 = predictions18.select(F.collect_list('prediction')).first()[0]\n",
    "pred20 = predictions19.select(F.collect_list('prediction')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\")\n",
    "#evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7777\n"
     ]
    }
   ],
   "source": [
    "print(len(pred20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from csv import writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def append_list_as_row(filename, elements):\n",
    "    with open(filename, 'a+', newline='') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(pred1)):\n",
    "    p = \"\"\n",
    "    p+=str(int(pred1[i]))\n",
    "    p+=\" \"+str(int(pred2[i]))\n",
    "    p+=\" \"+str(int(pred3[i]))\n",
    "    p+=\" \"+str(int(pred4[i]))\n",
    "    p+=\" \"+str(int(pred5[i]))\n",
    "    p+=\" \"+str(int(pred6[i]))\n",
    "    p+=\" \"+str(int(pred7[i]))\n",
    "    p+=\" \"+str(int(pred8[i]))\n",
    "    p+=\" \"+str(int(pred9[i]))\n",
    "    p+=\" \"+str(int(pred10[i]))\n",
    "    p+=\" \"+str(int(pred11[i]))\n",
    "    p+=\" \"+str(int(pred12[i]))\n",
    "    p+=\" \"+str(int(pred13[i]))\n",
    "    p+=\" \"+str(int(pred14[i]))\n",
    "    p+=\" \"+str(int(pred15[i]))\n",
    "    p+=\" \"+str(int(pred16[i]))\n",
    "    p+=\" \"+str(int(pred17[i]))\n",
    "    p+=\" \"+str(int(pred18[i]))\n",
    "    p+=\" \"+str(int(pred19[i]))\n",
    "    p+=\" \"+str(int(pred20[i]))\n",
    "    dict[movie_id[i]] = p\n",
    "    row_contents= [movie_id[i], p]\n",
    "    append_list_as_row(\"/home/cse587/Downloads/diccsvs/final.csv\", row_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TF = HashingTF(inputCol = 'plot_length', outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol = 'rawFeatures', outputCol=\"features\", minDocFreq=30)\n",
    "pipeline = Pipeline(stages=[TF, idf])\n",
    "pipefit = pipeline.fit(data)\n",
    "trainingData = pipefit.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8493040599183516\n",
      "0.9086759458677554\n",
      "0.9467678163875406\n",
      "0.960236587482722\n",
      "0.9721945417724774\n",
      "0.964640457745347\n",
      "0.9883956411327911\n",
      "0.9993892442701469\n",
      "0.9886206564016844\n",
      "0.9738339387315568\n",
      "0.9906457938217236\n",
      "0.9951782442380018\n",
      "0.9973641068501077\n",
      "0.9994856793853869\n",
      "0.9950175190459353\n",
      "0.9993892442701469\n",
      "0.9976212671574143\n",
      "0.9997106946542801\n",
      "0.9990999389244271\n",
      "0.9996785496158668\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_0', fitIntercept = True)\n",
    "lrModel = lr.fit(trainingData)\n",
    "print(lrModel.summary.accuracy)\n",
    "\n",
    "\n",
    "\n",
    "lr1 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_1', fitIntercept = True)\n",
    "lrModel1 = lr1.fit(trainingData)\n",
    "print(lrModel1.summary.accuracy)\n",
    "\n",
    "lr2 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_2', fitIntercept = True)\n",
    "lrModel2 = lr2.fit(trainingData)\n",
    "print(lrModel2.summary.accuracy)\n",
    "\n",
    "lr3 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_3', fitIntercept = True)\n",
    "lrModel3 = lr3.fit(trainingData)\n",
    "print(lrModel3.summary.accuracy)\n",
    "\n",
    "lr4 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_4', fitIntercept = True)\n",
    "lrModel4 = lr4.fit(trainingData)\n",
    "print(lrModel4.summary.accuracy)\n",
    "\n",
    "lr5 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_5', fitIntercept = True)\n",
    "lrModel5 = lr5.fit(trainingData)\n",
    "print(lrModel5.summary.accuracy)\n",
    "\n",
    "lr6 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_6', fitIntercept = True)\n",
    "lrModel6 = lr6.fit(trainingData)\n",
    "print(lrModel6.summary.accuracy)\n",
    "\n",
    "lr7 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_7', fitIntercept = True)\n",
    "lrModel7 = lr7.fit(trainingData)\n",
    "print(lrModel7.summary.accuracy)\n",
    "\n",
    "lr8 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_8', fitIntercept = True)\n",
    "lrModel8 = lr8.fit(trainingData)\n",
    "print(lrModel8.summary.accuracy)\n",
    "\n",
    "lr9 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_9', fitIntercept = True)\n",
    "lrModel9 = lr9.fit(trainingData)\n",
    "print(lrModel9.summary.accuracy)\n",
    "\n",
    "lr10 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_10', fitIntercept = True)\n",
    "lrModel10 = lr10.fit(trainingData)\n",
    "print(lrModel10.summary.accuracy)\n",
    "\n",
    "lr11 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_11', fitIntercept = True)\n",
    "lrModel11 = lr11.fit(trainingData)\n",
    "print(lrModel11.summary.accuracy)\n",
    "\n",
    "lr12 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_12', fitIntercept = True)\n",
    "lrModel12 = lr12.fit(trainingData)\n",
    "print(lrModel12.summary.accuracy)\n",
    "\n",
    "lr13 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_13', fitIntercept = True)\n",
    "lrModel13 = lr13.fit(trainingData)\n",
    "print(lrModel13.summary.accuracy)\n",
    "\n",
    "lr14 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_14', fitIntercept = True)\n",
    "lrModel14 = lr14.fit(trainingData)\n",
    "print(lrModel14.summary.accuracy)\n",
    "\n",
    "lr15 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_15', fitIntercept = True)\n",
    "lrModel15 = lr15.fit(trainingData)\n",
    "print(lrModel15.summary.accuracy)\n",
    "\n",
    "lr16 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_16', fitIntercept = True)\n",
    "lrModel16 = lr16.fit(trainingData)\n",
    "print(lrModel16.summary.accuracy)\n",
    "\n",
    "lr17 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_17', fitIntercept = True)\n",
    "lrModel17 = lr17.fit(trainingData)\n",
    "print(lrModel17.summary.accuracy)\n",
    "\n",
    "lr18 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_18', fitIntercept = True)\n",
    "lrModel18 = lr18.fit(trainingData)\n",
    "print(lrModel18.summary.accuracy)\n",
    "\n",
    "lr19 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_19', fitIntercept = True)\n",
    "lrModel19 = lr19.fit(trainingData)\n",
    "print(lrModel19.summary.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(vectorSize = 100, minCount = 3, inputCol=\"\", outputCol = \"\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
