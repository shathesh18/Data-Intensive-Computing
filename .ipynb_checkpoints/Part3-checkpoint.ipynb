{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, CountVectorizer\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Data preprocessing\") \\\n",
    "        .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "        .getOrCreate()\n",
    "dataframe = spark.read.csv(\"/home/cse587/Downloads/diccsvs/train.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n",
    "dataframe = dataframe.na.drop(subset=[\"genre\",\"plot\",\"movie_id\"])\n",
    "dataframe.printSchema()\n",
    "df_mapping = spark.read.csv(\"/home/cse587/Downloads/diccsvs/mapping.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clean text\n",
    "df_clean = dataframe.select('movie_id', 'movie_name', (lower(regexp_replace('plot',\"[^a-zA-Z\\\\s]\",\"\")).alias('plot')), (lower(regexp_replace(\"genre\",\"[^a-zA-Z\\-/,\\\\s]\",\"\")).alias(\"genre\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def replacelabel(x):\n",
    "    test = x.split(\", \")\n",
    "    num_label = []\n",
    "    if(len(test)<1):\n",
    "        return num_label\n",
    "    for label in test:\n",
    "        if label == 'drama':\n",
    "            num_label.append(0)\n",
    "        elif label == 'comedy':\n",
    "            num_label.append(1)\n",
    "        elif label == 'romance film':\n",
    "            num_label.append(2)\n",
    "        elif label ==  'thriller':\n",
    "            num_label.append(3)\n",
    "        elif label == 'action': \n",
    "            num_label.append(4)\n",
    "        elif label == 'world cinema':\n",
    "            num_label.append(5)\n",
    "        elif label == 'crime fiction':\n",
    "            num_label.append(6)\n",
    "        elif label == 'horror':\n",
    "            num_label.append(7)\n",
    "        elif label == 'black-and-white':\n",
    "            num_label.append(8)\n",
    "        elif label == 'indie':\n",
    "            num_label.append(9)\n",
    "        elif label == 'action/adventure':\n",
    "            num_label.append(10)\n",
    "        elif label == 'adventure':\n",
    "            num_label.append(11)\n",
    "        elif label == 'family film':\n",
    "            num_label.append(12)\n",
    "        elif label == 'short film':\n",
    "            num_label.append(13)\n",
    "        elif label == 'romantic drama':\n",
    "            num_label.append(14)\n",
    "        elif label == 'animation':\n",
    "            num_label.append(15)\n",
    "        elif label == 'musical':\n",
    "            num_label.append(16)\n",
    "        elif label == 'science fiction':\n",
    "            num_label.append(17)\n",
    "        elif label == 'mystery':\n",
    "            num_label.append(18)\n",
    "        elif label == 'romantic comedy':\n",
    "            num_label.append(19)\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_udf = udf(replacelabel, ArrayType(IntegerType()))\n",
    "df_clean = df_clean.withColumn('genre_value',label_udf(df_clean.genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- genre_value: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Tokenize Plot Text\n",
    "tokenizer = Tokenizer(inputCol = 'plot', outputCol = 'plot_token')\n",
    "df_words_token = tokenizer.transform(df_clean).select(\"movie_id\",\"movie_name\",\"plot_token\",\"genre\",\"genre_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove StopWords\n",
    "remover = StopWordsRemover(inputCol = 'plot_token', outputCol = 'plot_clean')\n",
    "df_words_token_rem_stopwor = remover.transform(df_words_token).select(\"movie_id\",\"movie_name\",\"plot_clean\",\"genre\",\"genre_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Text Stemming\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stem_udf = udf(lambda tokens : [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_words_token_rem_stopwor.withColumn(\"words_stemmed\" ,stem_udf(\"plot_clean\")).select('movie_id',\"words_stemmed\",\"genre\",\"genre_value\")\n",
    "df_stemmed = df_stemmed.withColumnRenamed(\"words_stemmed\",\"plot\")\n",
    "df_stemmed = df_stemmed.withColumnRenamed(\"genre_value\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stemmed = df_stemmed.withColumn(\"col_0\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_1\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_2\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_3\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_4\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_5\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_6\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_7\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_8\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_9\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_10\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_11\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_12\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_13\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_14\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_15\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_16\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_17\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_18\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_19\",lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name='movie_id', dataType=tp.StringType(), nullable=True),\n",
    "    tp.StructField(name='plot', dataType=tp.ArrayType(StringType()), nullable=True),\n",
    "    tp.StructField(name='genre', dataType=tp.StringType(), nullable=True),\n",
    "    tp.StructField(name='label', dataType=tp.ArrayType(IntegerType()), nullable=True),\n",
    "    tp.StructField(name='col_0', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_1', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_2', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_3', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_4', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_5', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_6', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_7', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_8', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_9', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_10', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_11', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_12', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_13', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_14', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_15', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_16', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_17', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_18', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_19', dataType=tp.IntegerType(), nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = df_stemmed.select(\"*\").toPandas()\n",
    "for index, row in result_df.iterrows():\n",
    "    label_arr = row['label']\n",
    "    for i in label_arr:\n",
    "        if i == 0:\n",
    "            result_df.loc[index, \"col_0\"] = 1\n",
    "        if i == 1:\n",
    "            result_df.loc[index, \"col_1\"] = 1\n",
    "        if i == 2:\n",
    "            result_df.loc[index, \"col_2\"] = 1\n",
    "        if i == 3:\n",
    "            result_df.loc[index, \"col_3\"] = 1\n",
    "        if i == 4:\n",
    "            result_df.loc[index, \"col_4\"] = 1\n",
    "        if i == 5:\n",
    "            result_df.loc[index, \"col_5\"] = 1\n",
    "        if i == 6:\n",
    "            result_df.loc[index, \"col_6\"] = 1\n",
    "        if i == 7:\n",
    "            result_df.loc[index, \"col_7\"] = 1\n",
    "        if i == 8:\n",
    "            result_df.loc[index, \"col_8\"] = 1\n",
    "        if i == 9:\n",
    "            result_df.loc[index, \"col_9\"] = 1\n",
    "        if i == 10:\n",
    "            result_df.loc[index, \"col_10\"] = 1\n",
    "        if i == 11:\n",
    "            result_df.loc[index, \"col_11\"] = 1\n",
    "        if i == 12:\n",
    "            result_df.loc[index, \"col_12\"] = 1\n",
    "        if i == 13:\n",
    "            result_df.loc[index, \"col_13\"] = 1\n",
    "        if i == 14:\n",
    "            result_df.loc[index, \"col_14\"] = 1\n",
    "        if i == 15:\n",
    "            result_df.loc[index, \"col_15\"] = 1\n",
    "        if i == 16:\n",
    "            result_df.loc[index, \"col_16\"] = 1\n",
    "        if i == 17:\n",
    "            result_df.loc[index, \"col_17\"] = 1\n",
    "        if i == 18:\n",
    "            result_df.loc[index, \"col_18\"] = 1\n",
    "        if i == 19:\n",
    "            result_df.loc[index, \"col_19\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stemmed = spark.createDataFrame(result_df, schema = my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movie_id='23890098', plot=['shlykov', 'hardwork', 'taxi', 'driver', 'lyosha', 'saxophonist', 'develop', 'bizarr', 'loveh', 'relationship', 'despit', 'prejudic', 'realiz', 'arent', 'differ'], genre='world cinema, drama', label=[5, 0], col_0=1, col_1=0, col_2=0, col_3=0, col_4=0, col_5=1, col_6=0, col_7=0, col_8=0, col_9=0, col_10=0, col_11=0, col_12=0, col_13=0, col_14=0, col_15=0, col_16=0, col_17=0, col_18=0, col_19=0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemmed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movie_id='23890098', plot=['shlykov', 'hardwork', 'taxi', 'driver', 'lyosha', 'saxophonist', 'develop', 'bizarr', 'loveh', 'relationship', 'despit', 'prejudic', 'realiz', 'arent', 'differ'], genre='world cinema, drama', label=[5, 0], col_0=1, col_1=0, col_2=0, col_3=0, col_4=0, col_5=1, col_6=0, col_7=0, col_8=0, col_9=0, col_10=0, col_11=0, col_12=0, col_13=0, col_14=0, col_15=0, col_16=0, col_17=0, col_18=0, col_19=0, plot_length=['shlykov', 'hardwork', 'taxi', 'driver', 'lyosha', 'saxophonist', 'develop', 'bizarr', 'loveh', 'relationship', 'despit', 'prejudic', 'realiz', 'arent', 'differ'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter words whose length is greater than 3\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) > 3], ArrayType(StringType()))\n",
    "df_stemmed = df_stemmed.withColumn('plot_length', filter_length_udf(col('plot')))\n",
    "data = df_stemmed.select(\"*\")\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "cv_model = Word2Vec(vectorSize=250, minCount = 5, inputCol=\"plot_length\", outputCol=\"features\")\n",
    "cv_model = cv_model.fit(df_stemmed)\n",
    "cv_result=cv_model.transform(df_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- plot: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- label: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- col_0: integer (nullable = true)\n",
      " |-- col_1: integer (nullable = true)\n",
      " |-- col_2: integer (nullable = true)\n",
      " |-- col_3: integer (nullable = true)\n",
      " |-- col_4: integer (nullable = true)\n",
      " |-- col_5: integer (nullable = true)\n",
      " |-- col_6: integer (nullable = true)\n",
      " |-- col_7: integer (nullable = true)\n",
      " |-- col_8: integer (nullable = true)\n",
      " |-- col_9: integer (nullable = true)\n",
      " |-- col_10: integer (nullable = true)\n",
      " |-- col_11: integer (nullable = true)\n",
      " |-- col_12: integer (nullable = true)\n",
      " |-- col_13: integer (nullable = true)\n",
      " |-- col_14: integer (nullable = true)\n",
      " |-- col_15: integer (nullable = true)\n",
      " |-- col_16: integer (nullable = true)\n",
      " |-- col_17: integer (nullable = true)\n",
      " |-- col_18: integer (nullable = true)\n",
      " |-- col_19: integer (nullable = true)\n",
      " |-- plot_length: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, OneVsRest\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7215596772638143\n",
      "0.7642161432382911\n",
      "0.8452537850782732\n",
      "0.8522614034523771\n",
      "0.868076762351731\n",
      "0.8621942203220933\n",
      "0.8989038541901058\n",
      "0.9377350605933974\n",
      "0.9040792053746505\n",
      "0.90504355652705\n",
      "0.9117297245170208\n",
      "0.9206017551190974\n",
      "0.9253592208042689\n",
      "0.9470249766948472\n",
      "0.9329133048313992\n",
      "0.954418335529911\n",
      "0.9372850300556109\n",
      "0.9566684882188434\n",
      "0.9468642515027805\n",
      "0.9462856408113408\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=40, elasticNetParam = 1, featuresCol = 'features', labelCol='col_0', fitIntercept = True)\n",
    "lrModel = lr.fit(cv_result)\n",
    "print(lrModel.summary.accuracy)\n",
    "\n",
    "lr1 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_1', fitIntercept = True)\n",
    "lrModel1 = lr1.fit(cv_result)\n",
    "print(lrModel1.summary.accuracy)\n",
    "\n",
    "lr2 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_2', fitIntercept = True)\n",
    "lrModel2 = lr2.fit(cv_result)\n",
    "print(lrModel2.summary.accuracy)\n",
    "\n",
    "lr3 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_3', fitIntercept = True)\n",
    "lrModel3 = lr3.fit(cv_result)\n",
    "print(lrModel3.summary.accuracy)\n",
    "\n",
    "lr4 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_4', fitIntercept = True)\n",
    "lrModel4 = lr4.fit(cv_result)\n",
    "print(lrModel4.summary.accuracy)\n",
    "\n",
    "lr5 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_5', fitIntercept = True)\n",
    "lrModel5 = lr5.fit(cv_result)\n",
    "print(lrModel5.summary.accuracy)\n",
    "\n",
    "lr6 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_6', fitIntercept = True)\n",
    "lrModel6 = lr6.fit(cv_result)\n",
    "print(lrModel6.summary.accuracy)\n",
    "\n",
    "lr7 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_7', fitIntercept = True)\n",
    "lrModel7 = lr7.fit(cv_result)\n",
    "print(lrModel7.summary.accuracy)\n",
    "\n",
    "lr8 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_8', fitIntercept = True)\n",
    "lrModel8 = lr8.fit(cv_result)\n",
    "print(lrModel8.summary.accuracy)\n",
    "\n",
    "lr9 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_9', fitIntercept = True)\n",
    "lrModel9 = lr9.fit(cv_result)\n",
    "print(lrModel9.summary.accuracy)\n",
    "\n",
    "lr10 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_10', fitIntercept = True)\n",
    "lrModel10 = lr10.fit(cv_result)\n",
    "print(lrModel10.summary.accuracy)\n",
    "\n",
    "lr11 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_11', fitIntercept = True)\n",
    "lrModel11 = lr11.fit(cv_result)\n",
    "print(lrModel11.summary.accuracy)\n",
    "\n",
    "lr12 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_12', fitIntercept = True)\n",
    "lrModel12 = lr12.fit(cv_result)\n",
    "print(lrModel12.summary.accuracy)\n",
    "\n",
    "lr13 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_13', fitIntercept = True)\n",
    "lrModel13 = lr13.fit(cv_result)\n",
    "print(lrModel13.summary.accuracy)\n",
    "\n",
    "lr14 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_14', fitIntercept = True)\n",
    "lrModel14 = lr14.fit(cv_result)\n",
    "print(lrModel14.summary.accuracy)\n",
    "\n",
    "lr15 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_15', fitIntercept = True)\n",
    "lrModel15 = lr15.fit(cv_result)\n",
    "print(lrModel15.summary.accuracy)\n",
    "\n",
    "lr16 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_16', fitIntercept = True)\n",
    "lrModel16 = lr16.fit(cv_result)\n",
    "print(lrModel16.summary.accuracy)\n",
    "\n",
    "lr17 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_17', fitIntercept = True)\n",
    "lrModel17 = lr17.fit(cv_result)\n",
    "print(lrModel17.summary.accuracy)\n",
    "\n",
    "lr18 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_18', fitIntercept = True)\n",
    "lrModel18 = lr18.fit(cv_result)\n",
    "print(lrModel18.summary.accuracy)\n",
    "\n",
    "lr19 = LogisticRegression(maxIter=20, elasticNetParam = 0.1, featuresCol = 'features', labelCol='col_19', fitIntercept = True)\n",
    "lrModel19 = lr19.fit(cv_result)\n",
    "print(lrModel19.summary.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.save(\"/home/cse587/DICNEWFinal/LGModel3-1/\")\n",
    "lrModel1.save(\"/home/cse587/DICNEWFinal/LGModel3-2/\")\n",
    "lrModel2.save(\"/home/cse587/DICNEWFinal/LGModel3-3/\")\n",
    "lrModel3.save(\"/home/cse587/DICNEWFinal/LGModel3-4/\")\n",
    "lrModel4.save(\"/home/cse587/DICNEWFinal/LGModel3-5/\")\n",
    "lrModel5.save(\"/home/cse587/DICNEWFinal/LGModel3-6/\")\n",
    "lrModel6.save(\"/home/cse587/DICNEWFinal/LGModel3-7/\")\n",
    "lrModel7.save(\"/home/cse587/DICNEWFinal/LGModel3-8/\")\n",
    "lrModel8.save(\"/home/cse587/DICNEWFinal/LGModel3-9/\")\n",
    "lrModel9.save(\"/home/cse587/DICNEWFinal/LGModel3-10/\")\n",
    "lrModel10.save(\"/home/cse587/DICNEWFinal/LGModel3-11/\")\n",
    "lrModel11.save(\"/home/cse587/DICNEWFinal/LGModel3-12/\")\n",
    "lrModel12.save(\"/home/cse587/DICNEWFinal/LGModel3-13/\")\n",
    "lrModel13.save(\"/home/cse587/DICNEWFinal/LGModel3-14/\")\n",
    "lrModel14.save(\"/home/cse587/DICNEWFinal/LGModel3-15/\")\n",
    "lrModel15.save(\"/home/cse587/DICNEWFinal/LGModel3-16/\")\n",
    "lrModel16.save(\"/home/cse587/DICNEWFinal/LGModel3-17/\")\n",
    "lrModel17.save(\"/home/cse587/DICNEWFinal/LGModel3-18/\")\n",
    "lrModel18.save(\"/home/cse587/DICNEWFinal/LGModel3-19/\")\n",
    "lrModel19.save(\"/home/cse587/DICNEWFinal/LGModel3-20/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.csv(\"/home/cse587/Downloads/diccsvs/test.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n",
    "test_df = test_df.na.drop(subset=[\"plot\",\"movie_id\"])\n",
    "test_df.printSchema()\n",
    "\n",
    "#clean text\n",
    "test_df_clean = test_df.select('movie_id', 'movie_name', (lower(regexp_replace('plot',\"[^a-zA-Z\\\\s]\",\"\")).alias('plot')))\n",
    "\n",
    "#Tokenize Plot Text\n",
    "test_df_words_token = tokenizer.transform(test_df_clean).select(\"movie_id\",\"movie_name\",\"plot_token\")\n",
    "\n",
    "#Remove StopWords\n",
    "test_df_words_token_rem_stopwor = remover.transform(test_df_words_token).select(\"movie_id\",\"movie_name\",\"plot_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Text Stemming\n",
    "test_df_stemmed = test_df_words_token_rem_stopwor.withColumn(\"words_stemmed\" ,stem_udf(\"plot_clean\")).select('movie_id',\"words_stemmed\")\n",
    "test_df_stemmed = test_df_stemmed.withColumnRenamed(\"words_stemmed\",\"plot\")\n",
    "\n",
    "test_df_stemmed = test_df_stemmed.withColumn('plot_length', filter_length_udf(col('plot')))\n",
    "test_data = test_df_stemmed.select(\"movie_id\",\"plot\",\"plot_length\")\n",
    "test_data = test_df_stemmed.select(\"movie_id\",\"plot\",\"plot_length\")\n",
    "#Word2Vec\n",
    "test_cv_result = cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>MODEL LOADING<<<<<<\n",
    "'''from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "savePath = './Part3/LGModel3'\n",
    "lrModel = LogisticRegressionModel.load(savePath + '-1')\n",
    "lrModel2 = LogisticRegressionModel.load(savePath + '-2')\n",
    "lrModel3 = LogisticRegressionModel.load(savePath + '-3')\n",
    "lrModel4 = LogisticRegressionModel.load(savePath + '-4')\n",
    "lrModel5 = LogisticRegressionModel.load(savePath + '-5')\n",
    "lrModel6 = LogisticRegressionModel.load(savePath + '-6')\n",
    "lrModel7 = LogisticRegressionModel.load(savePath + '-7')\n",
    "lrModel8 = LogisticRegressionModel.load(savePath + '-8')\n",
    "lrModel9 = LogisticRegressionModel.load(savePath + '-9')\n",
    "lrModel10 = LogisticRegressionModel.load(savePath + '-10')\n",
    "lrModel11 = LogisticRegressionModel.load(savePath + '-11')\n",
    "lrModel12 = LogisticRegressionModel.load(savePath + '-12')\n",
    "lrModel13 = LogisticRegressionModel.load(savePath + '-13')\n",
    "lrModel14 = LogisticRegressionModel.load(savePath + '-14')\n",
    "lrModel15 = LogisticRegressionModel.load(savePath + '-15')\n",
    "lrModel16 = LogisticRegressionModel.load(savePath + '-16')\n",
    "lrModel17 = LogisticRegressionModel.load(savePath + '-17')\n",
    "lrModel18 = LogisticRegressionModel.load(savePath + '-18')\n",
    "lrModel19 = LogisticRegressionModel.load(savePath + '-19')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test_cv_result)\n",
    "predictions1 = lrModel1.transform(test_cv_result)\n",
    "predictions2 = lrModel2.transform(test_cv_result)\n",
    "predictions3 = lrModel3.transform(test_cv_result)\n",
    "predictions4 = lrModel4.transform(test_cv_result)\n",
    "predictions5 = lrModel5.transform(test_cv_result)\n",
    "predictions6 = lrModel6.transform(test_cv_result)\n",
    "predictions7 = lrModel7.transform(test_cv_result)\n",
    "predictions8 = lrModel8.transform(test_cv_result)\n",
    "predictions9 = lrModel9.transform(test_cv_result)\n",
    "predictions10 = lrModel10.transform(test_cv_result)\n",
    "predictions11 = lrModel11.transform(test_cv_result)\n",
    "predictions12 = lrModel12.transform(test_cv_result)\n",
    "predictions13 = lrModel13.transform(test_cv_result)\n",
    "predictions14 = lrModel14.transform(test_cv_result)\n",
    "predictions15 = lrModel15.transform(test_cv_result)\n",
    "predictions16 = lrModel16.transform(test_cv_result)\n",
    "predictions17 = lrModel17.transform(test_cv_result)\n",
    "predictions18 = lrModel18.transform(test_cv_result)\n",
    "predictions19 = lrModel19.transform(test_cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0.0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions12.select('prediction').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate result\n"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "movie_id = predictions.select(F.collect_list('movie_id')).first()[0]\n",
    "pred1 = predictions.select(F.collect_list('prediction')).first()[0]\n",
    "pred2 = predictions1.select(F.collect_list('prediction')).first()[0]\n",
    "pred3 = predictions2.select(F.collect_list('prediction')).first()[0]\n",
    "pred4 = predictions3.select(F.collect_list('prediction')).first()[0]\n",
    "pred5 = predictions4.select(F.collect_list('prediction')).first()[0]\n",
    "pred6 = predictions5.select(F.collect_list('prediction')).first()[0]\n",
    "pred7 = predictions6.select(F.collect_list('prediction')).first()[0]\n",
    "print(\"intermediate result\")\n",
    "pred8 = predictions7.select(F.collect_list('prediction')).first()[0]\n",
    "pred9 = predictions8.select(F.collect_list('prediction')).first()[0]\n",
    "pred10 = predictions9.select(F.collect_list('prediction')).first()[0]\n",
    "pred11 = predictions10.select(F.collect_list('prediction')).first()[0]\n",
    "pred12 = predictions11.select(F.collect_list('prediction')).first()[0]\n",
    "pred13 = predictions12.select(F.collect_list('prediction')).first()[0]\n",
    "pred14 = predictions13.select(F.collect_list('prediction')).first()[0]\n",
    "pred15 = predictions14.select(F.collect_list('prediction')).first()[0]\n",
    "pred16 = predictions15.select(F.collect_list('prediction')).first()[0]\n",
    "pred17 = predictions16.select(F.collect_list('prediction')).first()[0]\n",
    "pred18 = predictions17.select(F.collect_list('prediction')).first()[0]\n",
    "pred19 = predictions18.select(F.collect_list('prediction')).first()[0]\n",
    "pred20 = predictions19.select(F.collect_list('prediction')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from csv import writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def append_list_as_row(filename, elements):\n",
    "    with open(filename, 'a+', newline='') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(pred1)):\n",
    "    p = \"\"\n",
    "    p+=str(int(pred1[i]))\n",
    "    p+=\" \"+str(int(pred2[i]))\n",
    "    p+=\" \"+str(int(pred3[i]))\n",
    "    p+=\" \"+str(int(pred4[i]))\n",
    "    p+=\" \"+str(int(pred5[i]))\n",
    "    p+=\" \"+str(int(pred6[i]))\n",
    "    p+=\" \"+str(int(pred7[i]))\n",
    "    p+=\" \"+str(int(pred8[i]))\n",
    "    p+=\" \"+str(int(pred9[i]))\n",
    "    p+=\" \"+str(int(pred10[i]))\n",
    "    p+=\" \"+str(int(pred11[i]))\n",
    "    p+=\" \"+str(int(pred12[i]))\n",
    "    p+=\" \"+str(int(pred13[i]))\n",
    "    p+=\" \"+str(int(pred14[i]))\n",
    "    p+=\" \"+str(int(pred15[i]))\n",
    "    p+=\" \"+str(int(pred16[i]))\n",
    "    p+=\" \"+str(int(pred17[i]))\n",
    "    p+=\" \"+str(int(pred18[i]))\n",
    "    p+=\" \"+str(int(pred19[i]))\n",
    "    p+=\" \"+str(int(pred20[i]))\n",
    "    dict[movie_id[i]] = p\n",
    "    row_contents= [movie_id[i], p]\n",
    "    append_list_as_row(\"/home/cse587/DICNEWFinal/output3.csv\", row_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
