{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, CountVectorizer,IDF\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Data preprocessing\") \\\n",
    "        .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "        .getOrCreate()\n",
    "dataframe = spark.read.csv(\"/home/cse587/shathesh/Assignment/dic487-587/train.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n",
    "dataframe = dataframe.na.drop(subset=[\"genre\",\"plot\",\"movie_id\"])\n",
    "dataframe.printSchema()\n",
    "df_mapping = spark.read.csv(\"/home/cse587/shathesh/Assignment/dic487-587/mapping.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean text\n",
    "df_clean = dataframe.select('movie_id', 'movie_name', (lower(regexp_replace('plot',\"[^a-zA-Z\\\\s]\",\"\")).alias('plot')), (lower(regexp_replace(\"genre\",\"[^a-zA-Z\\-/,\\\\s]\",\"\")).alias(\"genre\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replacelabel(x):\n",
    "    test = x.split(\", \")\n",
    "    num_label = []\n",
    "    if(len(test)<1):\n",
    "        return num_label\n",
    "    for label in test:\n",
    "        if label == 'drama':\n",
    "            num_label.append(0)\n",
    "        elif label == 'comedy':\n",
    "            num_label.append(1)\n",
    "        elif label == 'romance film':\n",
    "            num_label.append(2)\n",
    "        elif label ==  'thriller':\n",
    "            num_label.append(3)\n",
    "        elif label == 'action': \n",
    "            num_label.append(4)\n",
    "        elif label == 'world cinema':\n",
    "            num_label.append(5)\n",
    "        elif label == 'crime fiction':\n",
    "            num_label.append(6)\n",
    "        elif label == 'horror':\n",
    "            num_label.append(7)\n",
    "        elif label == 'black-and-white':\n",
    "            num_label.append(8)\n",
    "        elif label == 'indie':\n",
    "            num_label.append(9)\n",
    "        elif label == 'action/adventure':\n",
    "            num_label.append(10)\n",
    "        elif label == 'adventure':\n",
    "            num_label.append(11)\n",
    "        elif label == 'family film':\n",
    "            num_label.append(12)\n",
    "        elif label == 'short film':\n",
    "            num_label.append(13)\n",
    "        elif label == 'romantic drama':\n",
    "            num_label.append(14)\n",
    "        elif label == 'animation':\n",
    "            num_label.append(15)\n",
    "        elif label == 'musical':\n",
    "            num_label.append(16)\n",
    "        elif label == 'science fiction':\n",
    "            num_label.append(17)\n",
    "        elif label == 'mystery':\n",
    "            num_label.append(18)\n",
    "        elif label == 'romantic comedy':\n",
    "            num_label.append(19)\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_udf = udf(replacelabel, ArrayType(IntegerType()))\n",
    "df_clean = df_clean.withColumn('genre_value',label_udf(df_clean.genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenize Plot Text\n",
    "tokenizer = Tokenizer(inputCol = 'plot', outputCol = 'plot_token')\n",
    "df_words_token = tokenizer.transform(df_clean).select(\"movie_id\",\"movie_name\",\"plot_token\",\"genre\",\"genre_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove StopWords\n",
    "remover = StopWordsRemover(inputCol = 'plot_token', outputCol = 'plot_clean')\n",
    "df_words_token_rem_stopwor = remover.transform(df_words_token).select(\"movie_id\",\"plot_clean\",\"genre\",\"genre_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Text Stemming\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stem_udf = udf(lambda tokens : [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_words_token_rem_stopwor.withColumn(\"words_stemmed\" ,stem_udf(\"plot_clean\")).select('movie_id',\"words_stemmed\",\"genre\",\"genre_value\")\n",
    "df_stemmed = df_stemmed.withColumnRenamed(\"words_stemmed\",\"plot\")\n",
    "df_stemmed = df_stemmed.withColumnRenamed(\"genre_value\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemmed = df_stemmed.withColumn(\"col_0\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_1\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_2\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_3\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_4\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_5\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_6\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_7\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_8\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_9\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_10\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_11\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_12\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_13\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_14\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_15\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_16\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_17\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_18\",lit(0))\n",
    "df_stemmed = df_stemmed.withColumn(\"col_19\",lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name='movie_id', dataType=tp.StringType(), nullable=True),\n",
    "    tp.StructField(name='plot', dataType=StringType(), nullable=True),\n",
    "    tp.StructField(name='genre', dataType=tp.StringType(), nullable=True),\n",
    "    tp.StructField(name='label', dataType=tp.ArrayType(IntegerType()), nullable=True),\n",
    "    tp.StructField(name='col_0', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_1', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_2', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_3', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_4', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_5', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_6', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_7', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_8', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_9', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_10', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_11', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_12', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_13', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_14', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_15', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_16', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_17', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_18', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='col_19', dataType=tp.IntegerType(), nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.4 s, sys: 163 ms, total: 49.5 s\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_df = df_stemmed.select(\"*\").toPandas()\n",
    "for index, row in result_df.iterrows():\n",
    "    label_arr = row['label']\n",
    "    for i in label_arr:\n",
    "        if i == 0:\n",
    "            result_df.loc[index, \"col_0\"] = 1\n",
    "        if i == 1:\n",
    "            result_df.loc[index, \"col_1\"] = 1\n",
    "        if i == 2:\n",
    "            result_df.loc[index, \"col_2\"] = 1\n",
    "        if i == 3:\n",
    "            result_df.loc[index, \"col_3\"] = 1\n",
    "        if i == 4:\n",
    "            result_df.loc[index, \"col_4\"] = 1\n",
    "        if i == 5:\n",
    "            result_df.loc[index, \"col_5\"] = 1\n",
    "        if i == 6:\n",
    "            result_df.loc[index, \"col_6\"] = 1\n",
    "        if i == 7:\n",
    "            result_df.loc[index, \"col_7\"] = 1\n",
    "        if i == 8:\n",
    "            result_df.loc[index, \"col_8\"] = 1\n",
    "        if i == 9:\n",
    "            result_df.loc[index, \"col_9\"] = 1\n",
    "        if i == 10:\n",
    "            result_df.loc[index, \"col_10\"] = 1\n",
    "        if i == 11:\n",
    "            result_df.loc[index, \"col_11\"] = 1\n",
    "        if i == 12:\n",
    "            result_df.loc[index, \"col_12\"] = 1\n",
    "        if i == 13:\n",
    "            result_df.loc[index, \"col_13\"] = 1\n",
    "        if i == 14:\n",
    "            result_df.loc[index, \"col_14\"] = 1\n",
    "        if i == 15:\n",
    "            result_df.loc[index, \"col_15\"] = 1\n",
    "        if i == 16:\n",
    "            result_df.loc[index, \"col_16\"] = 1\n",
    "        if i == 17:\n",
    "            result_df.loc[index, \"col_17\"] = 1\n",
    "        if i == 18:\n",
    "            result_df.loc[index, \"col_18\"] = 1\n",
    "        if i == 19:\n",
    "            result_df.loc[index, \"col_19\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemmed = spark.createDataFrame(result_df, schema = my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter words whose length is greater than 3\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) > 0], ArrayType(StringType()))\n",
    "df_stemmed = df_stemmed.withColumn('plot_length', filter_length_udf(col('plot')))\n",
    "data = df_stemmed.select(\"*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.3 ms, sys: 491 µs, total: 61.8 ms\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_model = Word2Vec( vectorSize=100, seed=100, inputCol=\"plot_length\", outputCol=\"features\")\n",
    "cv_model = cv_model.fit(data)\n",
    "cv_result=cv_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "CPU times: user 637 ms, sys: 37 ms, total: 674 ms\n",
      "Wall time: 45min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_0\", seed=42)\n",
    "lrModel=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_1\", seed=42)\n",
    "lrModel1=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_2\", seed=42)\n",
    "lrModel2=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_3\", seed=42)\n",
    "lrModel3=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_4\", seed=42)\n",
    "lrModel4=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_5\", seed=42)\n",
    "lrModel5=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_6\", seed=42)\n",
    "lrModel6=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_7\", seed=42)\n",
    "lrModel7=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_8\", seed=42)\n",
    "lrModel8=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_9\", seed=42)\n",
    "lrModel9=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_10\", seed=42)\n",
    "lrModel10=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_11\", seed=42)\n",
    "lrModel11=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_12\", seed=42)\n",
    "lrModel12=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_13\", seed=42)\n",
    "lrModel13=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_14\", seed=42)\n",
    "lrModel14=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_15\", seed=42)\n",
    "lrModel15=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_16\", seed=42)\n",
    "lrModel16=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_17\", seed=42)\n",
    "lrModel17=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_18\", seed=42)\n",
    "lrModel18=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n",
    "RF=RandomForestClassifier(numTrees=3, maxDepth=10, featuresCol='features',labelCol=\"col_19\", seed=42)\n",
    "lrModel19=RF.fit(cv_result)\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-1\")\n",
    "lrModel1.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-2\")\n",
    "lrModel2.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-3\")\n",
    "lrModel3.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-4\")\n",
    "lrModel4.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-5\")\n",
    "lrModel5.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-6\")\n",
    "lrModel6.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-7\")\n",
    "lrModel7.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-8\")\n",
    "lrModel8.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-9\")\n",
    "lrModel9.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-10\")\n",
    "lrModel10.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-11\")\n",
    "lrModel11.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-12\")\n",
    "lrModel12.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-13\")\n",
    "lrModel13.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-14\")\n",
    "lrModel14.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-15\")\n",
    "lrModel15.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-16\")\n",
    "lrModel16.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-17\")\n",
    "lrModel17.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-18\")\n",
    "lrModel18.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-19\")\n",
    "lrModel19.save(\"/home/cse587/shathesh/Assignment/RFModel-bonus-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean text\n",
    "\n",
    "test_df = spark.read.csv(\"/home/cse587/shathesh/Assignment/dic487-587/test.csv\", escape =\"\\\"\", inferSchema = True, header = True)\n",
    "test_df = test_df.na.drop(subset=[\"plot\",\"movie_id\"])\n",
    "test_df.printSchema()\n",
    "test_df_clean = test_df.select('movie_id', 'movie_name', (lower(regexp_replace('plot',\"[^a-zA-Z\\\\s]\",\"\")).alias('plot')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Tokenize Plot Text\n",
    "test_df_words_token = tokenizer.transform(test_df_clean).select(\"movie_id\",\"movie_name\",\"plot_token\")\n",
    "\n",
    "#Remove StopWords\n",
    "test_df_words_token_rem_stopwor = remover.transform(test_df_words_token).select(\"movie_id\",\"movie_name\",\"plot_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Text Stemming\n",
    "\n",
    "test_df_stemmed = test_df_words_token_rem_stopwor.withColumn(\"words_stemmed\" ,stem_udf(\"plot_clean\")).select('movie_id',\"words_stemmed\")\n",
    "test_df_stemmed = test_df_stemmed.withColumnRenamed(\"words_stemmed\",\"plot\")\n",
    "test_df_stemmed = test_df_stemmed.withColumn('plot_length', filter_length_udf(col('plot')))\n",
    "test_data = test_df_stemmed.select(\"movie_id\",\"plot\",\"plot_length\")\n",
    "\n",
    "#transform data to word2vec\n",
    "test_cv_result = cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>MODEL LOADING<<<<<<\n",
    "'''from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "savePath = './Bonus/RFModel-bonus'\n",
    "lrModel = LogisticRegressionModel.load(savePath + '-1')\n",
    "lrModel2 = LogisticRegressionModel.load(savePath + '-2')\n",
    "lrModel3 = LogisticRegressionModel.load(savePath + '-3')\n",
    "lrModel4 = LogisticRegressionModel.load(savePath + '-4')\n",
    "lrModel5 = LogisticRegressionModel.load(savePath + '-5')\n",
    "lrModel6 = LogisticRegressionModel.load(savePath + '-6')\n",
    "lrModel7 = LogisticRegressionModel.load(savePath + '-7')\n",
    "lrModel8 = LogisticRegressionModel.load(savePath + '-8')\n",
    "lrModel9 = LogisticRegressionModel.load(savePath + '-9')\n",
    "lrModel10 = LogisticRegressionModel.load(savePath + '-10')\n",
    "lrModel11 = LogisticRegressionModel.load(savePath + '-11')\n",
    "lrModel12 = LogisticRegressionModel.load(savePath + '-12')\n",
    "lrModel13 = LogisticRegressionModel.load(savePath + '-13')\n",
    "lrModel14 = LogisticRegressionModel.load(savePath + '-14')\n",
    "lrModel15 = LogisticRegressionModel.load(savePath + '-15')\n",
    "lrModel16 = LogisticRegressionModel.load(savePath + '-16')\n",
    "lrModel17 = LogisticRegressionModel.load(savePath + '-17')\n",
    "lrModel18 = LogisticRegressionModel.load(savePath + '-18')\n",
    "lrModel19 = LogisticRegressionModel.load(savePath + '-19')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 165 ms, sys: 16.4 ms, total: 181 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = lrModel.transform(test_cv_result)\n",
    "predictions1 = lrModel1.transform(test_cv_result)\n",
    "predictions2 = lrModel2.transform(test_cv_result)\n",
    "predictions3 = lrModel3.transform(test_cv_result)\n",
    "predictions4 = lrModel4.transform(test_cv_result)\n",
    "predictions5 = lrModel5.transform(test_cv_result)\n",
    "predictions6 = lrModel6.transform(test_cv_result)\n",
    "predictions7 = lrModel7.transform(test_cv_result)\n",
    "predictions8 = lrModel8.transform(test_cv_result)\n",
    "predictions9 = lrModel9.transform(test_cv_result)\n",
    "predictions10 = lrModel10.transform(test_cv_result)\n",
    "predictions11 = lrModel11.transform(test_cv_result)\n",
    "predictions12 = lrModel12.transform(test_cv_result)\n",
    "predictions13 = lrModel13.transform(test_cv_result)\n",
    "predictions14 = lrModel14.transform(test_cv_result)\n",
    "predictions15 = lrModel15.transform(test_cv_result)\n",
    "predictions16 = lrModel16.transform(test_cv_result)\n",
    "predictions17 = lrModel17.transform(test_cv_result)\n",
    "predictions18 = lrModel18.transform(test_cv_result)\n",
    "predictions19 = lrModel19.transform(test_cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "movie_id = predictions.select(F.collect_list('movie_id')).first()[0]\n",
    "pred1 = predictions.select(F.collect_list('prediction')).first()[0]\n",
    "pred2 = predictions1.select(F.collect_list('prediction')).first()[0]\n",
    "pred3 = predictions2.select(F.collect_list('prediction')).first()[0]\n",
    "pred4 = predictions3.select(F.collect_list('prediction')).first()[0]\n",
    "pred5 = predictions4.select(F.collect_list('prediction')).first()[0]\n",
    "pred6 = predictions5.select(F.collect_list('prediction')).first()[0]\n",
    "pred7 = predictions6.select(F.collect_list('prediction')).first()[0]\n",
    "pred8 = predictions7.select(F.collect_list('prediction')).first()[0]\n",
    "pred9 = predictions8.select(F.collect_list('prediction')).first()[0]\n",
    "pred10 = predictions9.select(F.collect_list('prediction')).first()[0]\n",
    "pred11 = predictions10.select(F.collect_list('prediction')).first()[0]\n",
    "pred12 = predictions11.select(F.collect_list('prediction')).first()[0]\n",
    "pred13 = predictions12.select(F.collect_list('prediction')).first()[0]\n",
    "pred14 = predictions13.select(F.collect_list('prediction')).first()[0]\n",
    "pred15 = predictions14.select(F.collect_list('prediction')).first()[0]\n",
    "pred16 = predictions15.select(F.collect_list('prediction')).first()[0]\n",
    "pred17 = predictions16.select(F.collect_list('prediction')).first()[0]\n",
    "pred18 = predictions17.select(F.collect_list('prediction')).first()[0]\n",
    "pred19 = predictions18.select(F.collect_list('prediction')).first()[0]\n",
    "pred20 = predictions19.select(F.collect_list('prediction')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list_as_row(filename, elements):\n",
    "    with open(filename, 'a+', newline='') as write_obj:\n",
    "        csv_writer = writer(write_obj)\n",
    "        csv_writer.writerow(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(pred1)):\n",
    "    p = \"\"\n",
    "    p+=str(int(pred1[i]))\n",
    "    p+=\" \"+str(int(pred2[i]))\n",
    "    p+=\" \"+str(int(pred3[i]))\n",
    "    p+=\" \"+str(int(pred4[i]))\n",
    "    p+=\" \"+str(int(pred5[i]))\n",
    "    p+=\" \"+str(int(pred6[i]))\n",
    "    p+=\" \"+str(int(pred7[i]))\n",
    "    p+=\" \"+str(int(pred8[i]))\n",
    "    p+=\" \"+str(int(pred9[i]))\n",
    "    p+=\" \"+str(int(pred10[i]))\n",
    "    p+=\" \"+str(int(pred11[i]))\n",
    "    p+=\" \"+str(int(pred12[i]))\n",
    "    p+=\" \"+str(int(pred13[i]))\n",
    "    p+=\" \"+str(int(pred14[i]))\n",
    "    p+=\" \"+str(int(pred15[i]))\n",
    "    p+=\" \"+str(int(pred16[i]))\n",
    "    p+=\" \"+str(int(pred17[i]))\n",
    "    p+=\" \"+str(int(pred18[i]))\n",
    "    p+=\" \"+str(int(pred19[i]))\n",
    "    p+=\" \"+str(int(pred20[i]))\n",
    "    dict[movie_id[i]] = p\n",
    "    row_contents= [movie_id[i], p]\n",
    "    append_list_as_row(\"/home/cse587/shathesh/Assignment/preds_part_1.csv\", row_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
